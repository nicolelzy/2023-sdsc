{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://docs.google.com/uc?id=1tSEwB0qGJQatNAnntC3jVpuRQ1qV5TU8\" width=\"200\">\n",
    "\n",
    "# Introduction to Time Series \n",
    "\n",
    "Course Instructor: Nicole LEE ([LinkedIn](https://www.linkedin.com/in/nicoleleezhiying/))\n",
    "\n",
    "30 Mar 2023 (0900 - 1730)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Course Overview\n",
    "\n",
    "Time series analysis is a statistical method that deals with data collected over time. It involves analysing and modelling data to identify patterns and trends, and make predictions about future values in the series. Time series data can be found in a wide range of fields, including economics, finance, and engineering: \n",
    "\n",
    "- [IoT Sensor Predictions](https://www.timescale.com/blog/how-everactive-powers-a-dense-sensor-network-with-virtually-no-power-at-all/)\n",
    "- Predictions of Financial Markets\n",
    "- Prediction of macroeconomic factors which may contribute to business outcomes\n",
    "- Understand sales patterns and predict future sales and revenue\n",
    "\n",
    "<img align=\"center\" src=\"https://www.timescale.com/blog/content/images/2023/03/time-series-data_bitcoin_img2.png\" width=\"600\">\n",
    "\n",
    "In this 1-day course, you will learn about the basics of time series, data preparation of a time series dataset, visualization, modeling and forecasting to understand the underlying factors driving trends and provide impactful insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Learning Objectives\n",
    "\n",
    "1. [Introduction to Time Series](#sec_intro) <br>\n",
    "   1.1 [Stationarity and Tests for Stationarity](#subsec_stationarity) <br>\n",
    "   1.2 [Seasonality and Correlatation](#subsec_seasonality)<br>\n",
    "2. [Univariate Time Series Forecasting](#sec_univ)<br>\n",
    "   2.1 [Evaluation Metrics](#subsec_evaluation)<br>\n",
    "   2.2 [Moving Average](#subsec_ma)<br>\n",
    "   2.3 [Simple Exponential Smoothing, Holt's ES, Holt-Winter's ES](#subsec_es)<br>\n",
    "   2.4 [AR, MA, ARIMA, SARIMA, SARIMAX Models](#subsec_arima)<br>\n",
    "   2.4.1 [Summary](#subsec_uv_summary)<br>\n",
    "   2.6 [Fourier Transformations](#subsec_FFT)<br>\n",
    "3. [Multivariate Time Series Forecasting: VAR/VECM](#sec_multiv)<br>\n",
    "4. [Group Project](#sec_proj)<br>\n",
    "\n",
    "<!-- 1. [Basics](#sec_basics) <br> \n",
    "    1.1 [What is Folium, and what can it be used for?](#subsec_folium) <br>\n",
    "    1.2 [Plotting markers, circles and circle markers](#subsec_markers) <br> \n",
    "    1.3 [Adding images](#subsec_images) <br>\n",
    "    1.4 [Adding convenient tools](#subsec_tools) <br> \n",
    "<br>\n",
    "2. [S$1 Million Dollar Resale Flats in SG](#sec_resale) <br> \n",
    "    2.1 [Getting coordinates of flats from onemap API](#subsec_flatcoords) <br>\n",
    "    2.2 [Plotting flat locations using ClusterMap](#subsec_flatloc) <br>\n",
    "    2.3 [Plotting boundaries](#subsec_boundaries) <br>\n",
    "    2.4 [Plotting choropleths](#subsec_choropleths) <br>\n",
    "<br>\n",
    "3. [Timestamped Path](#sec_path) <br> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Other Exercises to Try\n",
    "\n",
    "1. Stock Price Prediction\n",
    "   - Pick a stock of interest\n",
    "   - Get price data from [Yahoo! Finance](https://sg.finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC)\n",
    "   - Identify characteristics of the time series\n",
    "   - Plot short-term predictions of stock prices\n",
    "\n",
    "2. HDB Pricing Trend\n",
    "   - Download HDB Data from [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)\n",
    "   - Consider breaking data points into its town boundaries, flat type, storey range, proximity to a facility, etc. and investigate trends in housing prices.\n",
    "    \n",
    "3. Find datasets on UCI/Kaggle and practice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='sec_intro'></a>\n",
    "### 1. Introduction to Time Series\n",
    "\n",
    "Time series is when a set of data points are chronologically collected over time. These allow us to identify changes of different types of data points over time. \n",
    "\n",
    "***\n",
    "<img align=\"center\" src=\"http://www.weather.gov.sg/wp-content/uploads/2020/03/Fig1.png\" width=\"600\">\n",
    "\n",
    "\n",
    "<u>Weather</u><br>\n",
    "For instance, we notice that the average annual temperatures have risen over the last 80 years. We can also overlay the rainfall dataset to compare the average rainfall per month over years, to study if there has been a change in the <i>monsoon season</i> over the past 5 years.<br>\n",
    "<img align=\"center\" src=\"images/Monthly_Total_Rain.png\" width=\"600\"><br>\n",
    "\n",
    "\n",
    "<u>Stock Market</u><br>\n",
    "<img align=\"center\" src=\"images/DJI_YahooFinanceChart.png\" width=\"600\"><br>\n",
    "Often, we see the stock prices rise and fall, reacting to various economic events. By stripping away the <i>noise</i> of the data, we will be able to gain a better understanding of the price trends or volatility trends. However, due to the various interactions between market participants, as well as concurrent micro and macro economic factors, it is often not possible to strip the stock prices data and understand the true drivers of a price surge or dip in prices. \n",
    "\n",
    "\n",
    "<u>Sale and Transaction Data</u><br>\n",
    "We will notice spikes in sales during a sale period. When you imagine a traditional retail market, you notice that the holiday period sees a peak, resulting in a clear yearly peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download and understand the components of a time series dataset\n",
    "rain_data = pd.read_csv(\"timeseries_sample.csv\")\n",
    "rain_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Characteristics of a Time Series\n",
    "A time series dataset is characterised by the following: <br>\n",
    "- Trend: A trend is a long-term pattern in the data that shows a consistent increase or decrease over time. Trends can be linear (or additive) or nonlinear (or multiplicative), and can be observed at different time scales.\n",
    "- Seasonality: Seasonality refers to regular, predictable patterns that repeat over a fixed time interval, such as daily, weekly, monthly or yearly. For example, sales of ice cream typically increase during summer months and decrease during winter months.\n",
    "- Cyclical: Cyclical patterns are similar to seasonality but occur over a longer time period, often several years. These patterns are often related to economic or business cycles.\n",
    "- Noise (or irregularity): Refers to fluctuations or randomness in the data that are not explained by trend, seasonality, or cyclical patterns. These \n",
    "\n",
    "<img align=\"center\" src=\"https://otexts.com/fpp2/fpp_files/figure-html/fourexamples-1.png\" width=\"600\"><br>\n",
    "These form the components of a time series.<br>\n",
    "<br>\n",
    "<b><u>Other terms which may be relevant include</u></b>:<br> \n",
    "<u>Lag</u><br>\n",
    "Refers to the time difference between two related variables or time delay between a dependent and independent variable. By understanding lag, it helps us understand how one unit of change in one variable can affect another variable over time.\n",
    "By studying the relationship of lag and its variables, we can identify the data trends across time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_stationarity'></a>\n",
    "\n",
    "#### 1.1 Stationarity and Tests for Stationarity\n",
    "Any time series dataset can be classified as stationary or non-stationarity, <br>\n",
    "- Stationary data refers to data where the statistical properties, such as mean and variance, remain constant over time. This means that the distribution of data points is consistent across all time periods, and there are no trends or patterns that change over time. Therefore, stationary time series or stationary processes are predictable. Analytical methodologies can be applied across all time periods in the data set. <br>\n",
    "- Non-stationary data refers to data whose statistical properties change over time. Hence, the trends, patterns or seasonality may differ and be difficult to predict over time. <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Stationarity can be further broken down into the following: <br>\n",
    "- <b>Strong/Strict/Complete Stationarity</b>: A process is <b>invariant</b> under a shift in time.<br>\n",
    "   This means all statistical properties are constant throughout time. \n",
    "- <b>Weak/Second-order/Covariance Stationarity</b>: A processes's first and second moments are invariant under a shift in time. <br>\n",
    "   So, the <u>mean</u> and <u>autocovariance</u> of a series are constant over time, but its variance may still change. <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Time series methodologies in this day course are most effective on stationarity data, which can be tested in 2 ways:<br>\n",
    "1. Visualization<br>\n",
    "2. Augmented Dickey Fuller Test<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<i>Mathematically, this is represented as: <br>\n",
    "1. Strong/Strict/Complete Stationarity<br>\n",
    "   Let us label a set of data points as \n",
    "   $$\\{X_t: t\\in T\\}$$\n",
    "   where $t$ is the index, or taken as time here and $T$ is the index set, which is the total time period we consider. $X$ can be univariate or multivariate, continuous or discrete. \n",
    "\n",
    "   Now, $\\{X_t\\}$ is said to be completely stationary, if for all $n \\geq 1$, for any $t_1, t_2, \\dots, t_n \\in T$ and for any lag $\\tau$ such that $t_1+\\tau, t_2+\\tau, \\dots, t_n+\\tau \\in T$ are also contained in the index set, the joint cdf $\\{X_{t_1}, X_{t_2}, \\dots, X_{t_n}\\}$ is the same as that of $\\{X_{t_{1+\\tau}}, X_{t_{2+\\tau}}, \\dots, X_{t_{n+\\tau}}\\}$ Then, \n",
    "   $$ F_{t_1, t_2, \\dots, t_n}(a_1, a_2, \\dots, a_n) = F_{t_(1+\\tau), t_(2+\\tau), \\dots, t_(n+\\tau)}(a_1, a_2, \\dots, a_n)$$\n",
    "   <br>\n",
    "   <b>A completely stationary process is invariant under a shift in time.</b><br>\n",
    "   <br>\n",
    "   <br>\n",
    "   <br>\n",
    "2. Weak/second-order Stationarity<br>\n",
    "   Now, $\\{X_t\\}$ is said to be second-order stationary, if for all $n \\geq 1$, for any $t_1, t_2, \\dots, t_n \\in T$ and for any lag $\\tau$ such that $t_1+\\tau, t_2+\\tau, \\dots, t_n+\\tau \\in T$ are also contained in the index set, all the joint moments of orders 1 and 2 of $\\{X_{t_1}, X_{t_2}, \\dots, X_{t_n}\\}$ exist, are finite, and equal to the corresponding joint moments of $\\{X_{t_{1+\\tau}}, X_{t_{2+\\tau}}, \\dots, X_{t_{n+\\tau}}\\}$ Then, \n",
    "   $$ E\\{X_t\\} \\equiv \\mu, \\qquad var\\{X_t\\} \\equiv \\sigma^2 = E\\{X_t^2\\} - \\mu^2$$\n",
    "   are constants independent of t. </i>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All time series data should have a time-index. Clean the dataset such that it has one\n",
    "value_col = \"Daily Rainfall Total (mm)\"\n",
    "# rain_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Consider the type of data and the appropriate size of a rolling window\n",
    "### Consider trying a few values to determine if the dataset is indeed stationary\n",
    "## Visual Stationarity Test\n",
    "rolling_window = 12\n",
    "rmean = rain_data[value_col].rolling(window=rolling_window).mean()\n",
    "rstd = rain_data[value_col].rolling(window=rolling_window).std()\n",
    "\n",
    "orig = plt.plot(rain_data[value_col], color=\"black\", label=\"Original\")\n",
    "mean = plt.plot(rmean, color=\"red\", label=\"Rolling Mean\")\n",
    "std = plt.plot(rstd, color=\"blue\", label=\"Rolling Standard Deviation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Rolling mean and standard deviation\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Augmented Dickey Fuller Test\n",
    "adftest = tsa.adfuller(rain_data[value_col], autolag=\"AIC\")  # Autolag AIC vs BIC\n",
    "dfoutput = pd.Series(\n",
    "    adftest[0:4],\n",
    "    index=[\"Test Statistic\", \"p-value\", \"#lags used\", \"number of observations used\"],\n",
    ")\n",
    "\n",
    "for key, value in adftest[4].items():\n",
    "    dfoutput[\"critical value (%s)\" % key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the series is non-stationary, often, it can be converted to a stationary series using a differencing method.\n",
    "We call this first layer of differencing as first differences, where $\\Delta Y_t = Y_t - Y_{t-1}$.<br>\n",
    "<br>\n",
    "$d$ = number of differencing steps it takes for a dataset to achieve stationarity.\n",
    "Note that $d=0$ indicates a stationary series and $d>0$ indicates a non-stationary series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Form a stationary dataset from the sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_seasonality'></a>\n",
    "\n",
    "#### 1.2 Seasonality\n",
    "\n",
    "- Seasonality refers to regular, predictable patterns that repeat over a fixed time interval, such as daily, weekly, monthly or yearly.\n",
    "- It can be caused by various factors, including seasons, holidays, weather, human behaviour, etc.\n",
    "- Often seasonality has different frequencies and the actual seasonality predicted from the historical data may differ from what one's expertise about a subject.\n",
    "\n",
    "<u>Methods to detect seasonality</u>\n",
    "There are multiple methods of detecting seasonality, and/or cyclicity\n",
    "- Visualize the data and not the periods of seasonality\n",
    "- Use autocorrelation function (ACF) and partial auto-correlation function (PACF) plots to determine presence of seasonality\n",
    "- Decompose time series into its trend, seasonality and residual components, allowing each component to be analysed individually"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>ACF and PACF plots</u><br>\n",
    "<br>\n",
    "<b>Autocorrelation Function (ACF)</b>: Measures how a series is correlated with itself at different lags.<br>\n",
    "<b>Partial Autocorrelation Function (PACF)</b> Regression of the series against its past lags. The terms can be interpreted as the effect of a change in the particular lag, while holding all other variables constant.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot ACF and PACF plots and take note of the lags and decay, which helps identify the components of the time series\n",
    "\n",
    "acf_pacf_axs = plt.subplots(2, 1)\n",
    "\n",
    "plot_acf(rain_data[value_col], ax=acf_pacf_axs[0])\n",
    "plot_pacf(rain_data[value_col], ax=acf_pacf_axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Time Series Decomposition</u><br>\n",
    "The time series can also be decomposed into its components, as stated above, i.e. its trend, seasonality and residuals. <br>\n",
    "In addition, the models can be classified as additive or multiplicative models:\n",
    "- Additive models: <br>\n",
    "  $$Observation = trend + seasonality + residual$$\n",
    "  The magnitude of the seasonal and residual components are independent of the trend.\n",
    "- Multiplicative models\n",
    "  $$ Observation = trend * seasonality * residual$$\n",
    "  The multiplicative model has seasonal and/or residual plots which tend to follow the trend.\n",
    "  They can be transformed into an additive model through a logarithmic transformation.\n",
    "  $$ \\therefore log(Observation) = log(time) + log(seasonality) + log(residual)\n",
    "- Pseudo-additive models (<i>Extra</i>)\n",
    "  Combines both additive and multiplicative models and are harder to spot. \n",
    "\n",
    "<i>Note:</i> Identifying the right model is important in finding the right method to remove seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS Decomposition\n",
    "\n",
    "decomposed_ts = tsa.seasonal_decompose(rain_data[value_col], freq=12)\n",
    "white_noise = decomposed_ts.resid\n",
    "print(\"Trend: {}\".format(decomposed_ts.trend))\n",
    "print(\"Seasonal: {}\".format(decomposed_ts.seasonal))\n",
    "print(\"White Noise: {}\".format(white_noise))\n",
    "print(\"Observed Values: {}\".format(decomposed_ts.observed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that white noise has\n",
    "- constant mean,\n",
    "- constant variance,\n",
    "- zero auto-correlation at all lags\n",
    "Note how the auto-correlation of white noise have insignificant lags and lie within the confidence interval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-seasonal data, represented by $(p, d, q)$ \n",
    "\n",
    "|       | ACF                          | PACF                         |\n",
    "|-------|------------------------------|------------------------------|\n",
    "| AR(p) | Geometric Decay              | Significant till $p$ lags    |\n",
    "| MA(q) | Significant till $q$ lags    | Geometric Decay              |\n",
    "\n",
    "For seasonal data, represented by $(P, D, Q)m$\n",
    "|       | ACF                          | PACF                         |\n",
    "|-------|------------------------------|------------------------------|\n",
    "| AR(p) | Geometric Decay              | Significant at $m$ lags      |\n",
    "| MA(q) | Significant at $m$ lags      | Geometric Decay              |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='sec_univ'></a>\n",
    "### 2. Univariate Time Series Forecasting Methods\n",
    "\n",
    "Given that we now know stationarity, and seasonality and the ways to detect then, we want to forecast the data based on its historical trends. \n",
    "\n",
    "We explore several statistical techniques in this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_evaluation'></a>\n",
    "#### 2.1 Evaluation Metrics\n",
    "\n",
    "- MSE or Root MSE (RMSE)<br>\n",
    "  $$\\frac{1}{T} \\sum_{t=1}^T (F_t - Y_t)^2, $$\n",
    "  where $F_t$ is the forecasted value at time $t$ and $Y_t$ the actual value at time $t$.\n",
    "\n",
    "- Mean absolute deviation (MAD)<br>\n",
    "  $$\\frac{1}{T}\\sum_{t=1}^T |F_t - Y-t|,$$\n",
    "  where $F_t$ is the forecasted value at time $t$ and $Y_t$ the actual value at time $t$.\n",
    "\n",
    "- Mean absolute percentage error (MAPE)<br>\n",
    "  $$\\frac{1}{T}\\sum_{t=1}^T |\\frac{F_t - Y-t}{Y_t}|,$$\n",
    "  where $F_t$ is the forecasted value at time $t$ and $Y_t$ the actual value at time $t$.\n",
    "\n",
    "MAPE is unit free, and therefore ideal for comparing models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "<a id='subsec_ma'></a>\n",
    "\n",
    "#### 2.2 Moving Average: MA(n)\n",
    "Method used to smooth out fluctuations, also known as the \"rolling average\", therefore removing the effects of seasonality. \n",
    "\n",
    "The number of periods used to compute the moving/rolling average is known as the \"window size\" or \"lag\".\n",
    "\n",
    "\n",
    "As n gets larger, the model is more robust and reflects the overall trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use pd.DataFrame.rolling() to experiment with the moving averages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_es'></a>\n",
    "#### 2.3 Exponential Smoothing Methods (ES)\n",
    "<u>Simple Exponential Smoothing (SES)</u><br>\n",
    "$$ F_{t+1} = \\alpha Y_t + (1 - \\alpha)F_t,$$\n",
    "where $\\alpha$ is the smoothing parameter, smaller $\\alpha$ = more sensitive to recent data changes.\n",
    "\n",
    "- Gives more weight to recent observations, and less weight to older observations<br>\n",
    "- Utilises a weighted average, instead of a simple average in MA<br>\n",
    "- As events get older, the weights get exponentially smaller<br>\n",
    "- Particularly useful when there is no clear seasonality<br>\n",
    "\n",
    "\n",
    "<u> Holt's Exponential Smoothing (Holt's ES)</u><br>\n",
    "$$ F_{t+p} = L_t + pT_t$$\n",
    "$$ L_t = \\alpha Y_t + (1 - \\alpha)(L_{t-1} + T_{t-1})$$\n",
    "$$ T_t = \\beta(L_t - L_{t-1}) + (1 - \\beta)T_{t-1},$$\n",
    "\n",
    "where $F_{t+p}$ is a forecasted value for p time periods away, $L_t$ is the level (or intercept) for time $t$, and $T_t$ is the trend (or slope) at time $t$. <br>\n",
    "\n",
    "- Extension of SES, taking into account the linear trend<br>\n",
    "- <b>Two</b> smoothing parameters, one for level and other for trend<br>\n",
    "- Particularly useful for data with a clear trend but no clear seasonality<br>\n",
    "\n",
    "\n",
    "<u>Winter's Exponential Smoothing</u><br>\n",
    "$$L_t = \\alpha \\frac{Y_t}{S_{t-s}} + (1-\\alpha)(L_{t-1} + T_{t-1})$$\n",
    "$$S_t = \\gamma \\frac{Y_t}{L_t} + (1 - \\gamma) S_{t-s}$$\n",
    "$$ T_t = \\beta(L_t - L_{t-1}) + (1 - \\beta)T_{t-1}$$\n",
    "$$F_{t+p} = (L_t + pT_t)S_{t-s+p}$$\n",
    "- Extends from Holt's ES, by adding a seasonal component.<br>\n",
    "- <b>Three</b> smoothing parameters, $\\alpha, \\beta, \\gamma$ for level, trend and seasonality respectively. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_train = pd.read_csv(\"goog_train.csv\")\n",
    "goog_test = pd.read_csv(\"goog_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SES is suitable for data with no trend or seasonal pattern.\n",
    "### forecast forward 100 steps with smoothing_level = 0.2\n",
    "\n",
    "\n",
    "### Plot Original values and its forecasted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SES should not be used on data with a trend or seasonal component.\n",
    "### Practice differencing to remove trends. Consider using .diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import various metrics from sklearn and experiment with them\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MAD_SES = mean_absolute_error(\n",
    "#     forecasted, actual\n",
    "# )\n",
    "# print(\"MAD of Simple Exponential Smoothing on differenced time series:\", MAD_SES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Holt's ES and forecast forward 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try your hand at Holt-Winter's ES and forecast forward 100 steps.\n",
    "### Try using ExponentialSmoothing(train,trend='add', seasonal='add',seasonal_periods = 4).fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_arima'></a>\n",
    "#### 2.4 Models for Time Series datasets\n",
    "Autoregressive Integrated Moving Average (ARIMA) - Linear models which can represent time series data (including non-stationary)<br>\n",
    "ARIMA models do not involve independent variables and use information in the series to generate forecasts.<br>\n",
    "\n",
    "\n",
    "<u>Autoregressive (AR) Model</u><br>\n",
    "$AR(p)$: Uses previous $p$ (also called order of AR model) values to predict the future values\n",
    "$$ Y_t = \\phi_0 + \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t,$$\n",
    "where $\\epsilon_t$ represents the white noise, $\\{\\phi_i\\}_{i=1}^p$ are coefficients to estimate, and $Y_{i}$ are the values at time $i$\n",
    "<i>Stationary time series can be represented by $AR(1)$</i><br>\n",
    "\n",
    "\n",
    "<u>Moving Average (MA) Model</u><br>\n",
    "$MA(q)$: Uses previous $p$ (also called order of AR model) error terms to predict the future values\n",
    "$$ Y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q} \\implies Y_t - \\mu = \\epsilon_t + \\sum_{i = 1}^q \\theta_i \\epsilon_{t-i},$$\n",
    "where $\\epsilon_t$ represents the white noise, $\\{\\theta_i\\}_{i=1}^q$ are coefficients to estimate, and $Y_{i}$ are the values at time $i$<br>\n",
    "The deviation of the actual value at time t, is equal to the sum of current and past errors.\n",
    "\n",
    "\n",
    "<i>Consider $MA(1) \\equiv AR(\\infty)$</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AR model\n",
    "train_data = ...\n",
    "test_data = ...\n",
    "\n",
    "### Fit AR model\n",
    "model_ar = tsa.AR(train_data)\n",
    "model_ar_fit = model_ar.fit()\n",
    "\n",
    "### Make Predictions\n",
    "ar_predictions = model_ar_fit.predict(\n",
    "    start=len(train_data), end=len(train_data) + len(test_data) - 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Mean Squared Error for a MA model is {}\".format(\n",
    "        mean_squared_error(test_data, ar_predictions)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MA Model\n",
    "model_ma = tsa.ARMA(train_data)\n",
    "model_ma_fit = model_ma.fit()\n",
    "\n",
    "ma_predictions = model_ma_fit.predict(\n",
    "    start=len(train_data), end=len(train_data) + len(test_data) - 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Mean Squared Error for a MA model is {}\".format(\n",
    "        mean_squared_error(test_data, ma_predictions)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>ARIMA Model</u><br>\n",
    "<b>$ARIMA(p,d,q)$</b>: Combines AR and MA models, where \n",
    "  - $p$: Order of the AR model, \n",
    "  - $d$: Order of differencing (to make time series stationary),\n",
    "  - $q$: Order of MA model\n",
    "\n",
    "\n",
    "<u>SARIMA Model</u><br>\n",
    "<b>$SARIMA(p,d,q)\\times(P,D,Q)_s$</b>: Extension of ARIMA model which accounts for seasonality. AR and MA models, where \n",
    "  - $p$: Order of the AR model, \n",
    "  - $d$: Order of differencing (to make time series stationary),\n",
    "  - $q$: Order of MA model\n",
    "  - $P$: Seasonal order of AR model,\n",
    "  - $D$: Seasonal order of differencing,\n",
    "  - $Q$: Seasonal order of MA model\n",
    "  - $s$: Seasonal period\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "Additionally, it would be good to know that the following models which account for external factors exist but will not be covered in today's session<br>\n",
    "<u><i>ARIMAX Model</i></u>\n",
    "<b>$ARIMAX(p,d,q,x)$</b>: \"X\" stands for Exogenous Variables, which extends the ARIMA model to account for the impact of external variables on the time series, where x represents the external variable.\n",
    "\n",
    "\n",
    "<u><i>SARIMAX Model</i></u>\n",
    "u>SARIMA Model</u><br>\n",
    "<b>$SARIMA(p,d,q)\\times(P,D,Q)_s$</b>: Extension of ARIMAX model which accounts for seasonality. AR and MA models, where \n",
    "  - $p$: Order of the AR model, \n",
    "  - $d$: Order of differencing (to make time series stationary),\n",
    "  - $q$: Order of MA model\n",
    "  - $P$: Seasonal order of AR model,\n",
    "  - $D$: Seasonal order of differencing,\n",
    "  - $Q$: Seasonal order of MA model\n",
    "  - $s$: Seasonal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ARIMA Model\n",
    "model_arima = tsa.ARIMA()\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "arima_predictions = model_arima_fit.predict(\n",
    "    start=len(train_data), end=len(train_data) + len(test_data) - 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Mean Squared Error for a ARIMA model is {}\".format(\n",
    "        mean_squared_error(test_data, arima_predictions)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SARIMA Model\n",
    "model_sarima = tsa.SARIMA(train_data, order=..., seasonal_order=...)\n",
    "model_sarima_fit = model_sarima.fit()\n",
    "\n",
    "sarima_predictions = model_sarima_fit.predict(\n",
    "    start=len(train_data), end=len(train_data) + len(test_data) - 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Mean Squared Error for a SARIMA model is {}\".format(\n",
    "        mean_squared_error(test_data, sarima_predictions)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can also consider installing pmdarima, which is the Python's version of R's auto.arima\n",
    "# !pip install pmdarima\n",
    "\n",
    "### from pmdarima install auto_arima"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Activity\n",
    "1. Ingest the tractor sales dataset\n",
    "2. Remove trends, for both mean and variance\n",
    "3. Plot ACF and PACF to identify potential AR and MA model\n",
    "4. Check `model.summary()`\n",
    "5. Forecast sales using the best fit ARIMA model\n",
    "6. Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_uv_summary'></a>\n",
    "#### 2.4.1 Univariate Time Series Techniques Summary\n",
    "\n",
    "1. Determine whether the series is stationary<br>\n",
    "2. Plot ACF, PACF to determine seasonality<br>\n",
    "3. Conduct differencing if non-stationary, iterate until stationarity is achieved<br>\n",
    "4. Compare lags in ACF and PACF plots to identify the form of the model<br>\n",
    "5. Use of evaluation metrics to evaluate the models - MSE, MAD, MAPE, Akaike's Information Criterion (AIC), Bayesian Information Criterion (BIC)<br>\n",
    "      - Recall: Principle of parsimony in Data Science (BIC accounts for number of variables)<br>\n",
    "6. Ensure residuals are random<br>\n",
    "      - Ljung-Box Q Statistics: Plot ACF and PACF of the <b><u>residuals</u></b>. Approximately distributed as a $\\chi$-square random variable with m-r degrees of freedom where r is the total number of parameters estimated in the ARIMA model<br>\n",
    "7. Recall that forecasts further into the horizon are subjected to higher uncertainty. Forecasts should be recomputed as more data becomes available. This should be factored into the model monitoring process, which is not covered in today's session<br>\n",
    "\n",
    "\n",
    "|Pros|Cons|\n",
    "|--|---|\n",
    "|Provide accurate short-range forecasts|A relatively large amount of data is required (> 50 for non-seasonal data and 6-10 years of seasonal data)|\n",
    "Quite flexible and can represent a wide range of characteristics of time series|No easy way to update the parameters as new data become available|\n",
    "Formal procedures for testing the adequacy of the model are available||\n",
    "Forecasts and prediction intervals follow directly from the fitted model||"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='subsec_FFT'></a>\n",
    "#### 2.5 Fourier Transformations (!Technical!)\n",
    "\n",
    "- Fourier Transformations decomposes any function into sines and cosines (or waves)\n",
    "- Widely used in time series analysis for understanding the periodicity and seasonality of a signal\n",
    "- Converts time-domain to frequency-domain, which is obtained by decomposing it into a sum of sinusoidal functions of different frequencies, each of its own amplitude and phase. \n",
    "\n",
    "\n",
    "<u>Discrete Fourier Transform</u><br>\n",
    "- Converts data to sequence of discrete frequencies.\n",
    "- Use `np.fft` (Fast Fourier Transform) to compute the DFT. FFT is a faster version of the DFT from an algorithmic standpoint.\n",
    "- The values at which the power spectrum plot peaks, demonstrates the strength of each frequency component. This translates loosely to the periodicity of a signal, or seasonality in time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply FFT and take absolute values\n",
    "fft = np.abs(np.fft.fft(train_data))\n",
    "\n",
    "### Compute the power spectrum\n",
    "power = fft**2\n",
    "\n",
    "### Compute the frequencies\n",
    "freq = np.fft.fftfreq(len(power), d=1)  # assumes index is 1 unit apart here\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(freq, power)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='sec_multiv'></a>\n",
    "### 3. Multivariate Time Series\n",
    "Multivariate time series analysis, as its name suggests, has multiple time-dependent variables, i.e.the values of variable X at time $t$ may be dependent on variable $Y$ at time $t-1$. <br>\n",
    "Vector Autoregression (VAR) and Vector Error Correction Model (VECM) are two popular techniques for analyzing multivariate time series data.<br>\n",
    "\n",
    "\n",
    "\n",
    "<u>VAR Model</u><br>\n",
    "Generalizes the univariate AR model, to accomodate multiple time series, modeled as a system of interdependent linear equations. <br>\n",
    "Assumption: <br>\n",
    "1. Each variable is stationary: Each variable is a function of its own lagged values, \n",
    "2. There exists a linear relationship between their current and past values<br>\n",
    "Use of <i>Granger Causality tests</i> can determine if one variable can beused as a predictor of another variable.\n",
    "\n",
    "\n",
    "<u>VECM Model</u><br>\n",
    "Extends VAR to accomodate non-stationary time series data. <br>\n",
    "Variables are first differenced to till stationary, and then the VAR model is run. <br>\n",
    "Includes an error correction term, which captures the long-run equilibrium relationship among the variables.<br> \n",
    "Useful for data with cointegration, even if the variables are not directly causally related<br>\n",
    "\n",
    "\n",
    "VAR and VECM models are estimated using AIC nad BIC criteria. (Lower AIC = Better Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-430eea106cd3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-430eea106cd3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    X_train_log_diff[['Gold','Silver']], maxlag=15, addconst=True, verbose=True))\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "\n",
    "### GC Test\n",
    "print(grangercausalitytests(\n",
    "    train_data[[:2]] \n",
    "    maxlag=15, addconst=True, verbose=True))\n",
    "\n",
    "### VAR Model\n",
    "VAR_model = tsa.VAR(endog=train_data)\n",
    "VAR_results = VAR_model.fit(maxlags=2, ic='aic')\n",
    "\n",
    "### Forecast using VAR model\n",
    "lag_order = VAR_results.k_ar\n",
    "forecast_input = train_data.values[-lag_order:]\n",
    "var_fc = VAR_results.forecast(y=forecast_input, steps=len(test_data))\n",
    "\n",
    "print(VAR_results.summary())\n",
    "\n",
    "### Compute RMSE of VAR model\n",
    "var_rmse = rmse(var_fc, test_data.values)\n",
    "\n",
    "print(f\"VAR RMSE: {var_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec_rank1 = vecm.select_coint_rank(\n",
    "    train_data, det_order = 1, k_ar_diff = 1, method = 'trace', signif=0.01\n",
    "    ) #methods = trace, maxeig\n",
    "\n",
    "### VECM model\n",
    "vecm_model = tsa.VECM(train_data, k_rank=1, deterministic='ci')\n",
    "vecm_results = vecm_model.fit()~\n",
    "\n",
    "### Forecast using VECM model\n",
    "vecm_fc = vecm_results.predict(steps=len(test_data))\n",
    "\n",
    "### Compute RMSE of VECM model\n",
    "vecm_rmse = rmse(vecm_fc, test_data.values)\n",
    "\n",
    "\n",
    "print(f\"VECM RMSE: {vecm_rmse:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"sec_proj\"></a>\n",
    "### 4. Group Project\n",
    "Consider one of the following datasets:<br>\n",
    "- Beijing Multi-Site Air Quality Data,<br>\n",
    "- Pasir Panjang Rainfall Data, (or other rainfall data from [Meteorological Service Singapore](http://www.weather.gov.sg/climate-historical-daily/)) <br>\n",
    "- HDB Resale Prices <br>\n",
    "- Stock data from Yahoo! Finance<br>\n",
    "\n",
    "or any dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=10to100&numIns=&type=ts&sort=nameUp&view=table), [The UEA & UCR Time Series Classification Repository](https://www.timeseriesclassification.com/)\n",
    "\n",
    "Using the time series dataset, conduct a time series decomposition and analysis of the dataset. \n",
    "\n",
    "At the end of the session, present your findings of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Helpful Resources:\n",
    "- [Forecasting: Principles and Practice](https://otexts.com/fpp2/)<br>\n",
    "- [Jeffrey Yau: Time Series Forecasting using Statistical and Machine Learning Models](https://www.youtube.com/watch?v=_vQ0W_qXMxk)<br>\n",
    "- [A Very Short Course on Time Series Analysis: Fourier Transform](https://bookdown.org/rdpeng/timeseriesbook/the-fourier-transform.html) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT5151",
   "language": "python",
   "name": "bt5151"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
